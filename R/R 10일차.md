# R 10일차

## 1. 의사 결정 트리(Decision Tree)

- 의사 결정 트리(Decision Tree)

  - 나무(Tree) 구조 형태로 분류결과를 도출
  - 입력변수 중 가장 영향력 있는 변수를 기준으로 이진분류하여 분류 결과를 나무 구조 형태로 시각화
  - 비교적 모델 생성이 쉽고, 단순, 명료하여 현업에서 많이 사용되는 지도학습 모델
  - 의사결정규칙을 도표화 하여 분류와 예측을 수행하는 분석방법
  - party 패키지 ctree()
  - rpart 패키지 rpart()

  

  - party 패키지 ctree()  분류 결과 해석
  - 첫번째 번호는 반응변수(종속변수)에 대해서 설명변수(독립변수)가 영향을 미치는 중요 변수의 척도를 나타내는 수치로서 수치가 작을 수록 영향을 미치는 정도가 높고, 순서는 분기되는 순서를 의미한다.
  - 두번째는 의사결정 트리의 노드명 (노드 번호 뒤에 * 기호가 오면 해당 노드가 마지막 노드를 의미)
     노드명 뒤에 해당 변수의 임계값이 조건식으로 온다
  - 세번째는 노드의 분기 기준(criterion)이 되는 수치
  - 네번째는 반응변수(종속변수)의 통계량(statistic)이 표시된다. 

```R
install.packages("party")
library(party)
library(datasets)
str(airquality) # 관측 154개, 변수 6개

formula <- Temp ~ Solar.R+Wind+Ozone
air_ctree <- ctree(formula, data=airquality)
air_ctree
plot(air_ctree)
```

![1569199275517](assets/1569199275517.png)

```R
#온도에 가장 큰 영향을 미치는 첫번째 영향변수는 Ozone
# 두번째 영향변수는 Wind
# 오존량 37이하이면서 바람의 양이 15.5이상이면 평균온도는 63정도에 해당
#바람의 양이 15.5이하인 경우 평균 온도는 70이상으로 나타남
#태양광은 온도에 영향을 미치지 않는 것으로 분석됨
```



## 2. iris 데이터 셋으로 분류 분석 수행

```R
set.seed(1234) # 시드값을 적용하면 랜덤 값이 동일하게 생성
idx <- sample(1:nrow(iris), nrow(iris)*0.7)
train <- iris[idx, ] # 학습 데이터
test <- iris[-idx, ] # -는 제외, 검정 데이터

# 종속변수는 Species, 독립변수는 나머지
formula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width #(~는 영향을 주는 요소)

#분류모델 생성
iris_ctree <- ctree(formula, data=train)
iris_ctree
plot(iris_ctree, type="simple")
```

![1569199797839](assets/1569199797839.png)

```R
# 꽃종 분류에 가장 중요한 독립변수는? Petal.Length, Petal.Width

# 분류모델 평가 - 예측치 생성, 혼돈 매트릭스 생성

pred <- predict(iris_ctree, test)
table(pred, test$Species) # 예측값이 행, Species가 열

#분류 정확도 : ( 16 + 15 + 12) / nrow(test)
> ( 16 + 15 + 12) / nrow(test)
[1] 0.9555556 / 95.56%

> nrow(test)
[1] 45
```



## 3. k겹 교차 검증(k-fold cross validation)

```R
# 3겹, 2회 반복을 위한 샘플링
install.packages("cvTools")
library(cvTools)
cross <- cvFolds(nrow(iris), K=3, R=2)
str(cross)

cross #교차검정 데이터 확인
length(corss$which)
dim(cross$subsets)
table(cross$which)

R=1:2	#2회 반복
K=1:3	#k겹(3겹)
CNT=0	#카운트 변수
ACC <- numeric() #정확도 저장

for(r in R){
    cat('\n R=',r,'\n')
    for(k in K){
        datas_idx <- cross$subsets[cross$which=k, r]
        test <- iris[datas_idx, ] # test 데이터 생성
        cat('test:', nrow(test), '\n')
        
        formula <- Species ~ . # 나머지 모든 변수
        train <- iris[-dats_dix, ] # test행 뺀 훈련데이터
        cat('train:', nrow(train), '\n')
        
        model <- ctree(formula, data=train)
        pred <- predict(model, test)
        t <- table(pred, test$Species)
        print(t)
        CNT <- CNT+1
        ACC[CNT] <- (t[1,1]+t[2,2]+t[3,3])/sum(t)
        
    }
}
CNT #테스트 데이터 3셋 생성 모델, 예측 비교를 2번 반복, 즉 6회 수행
ACC #6회 수행 정확도 확인
#6회 정확도의 평균
mean(ACC, na.rm=T)

```

### 1. 연습문제1

```R
# 분류분석 연습문제
# ggplot2 :: mpg 데이터 셋
# model(모델), displ(엔진 크기), cyl(실린더 수), drv(구동 방식)
# 종속 변수 : 고속도로 주행거리(hwy)
# 고속도로 주행 거리에 가장 영향을 많이 미치는 요소 찾기

library(ggplot2)
data(mpg)
t <-sample(1:nrow(mpg), 120)
train <- mpg[t, ]
test <- mpg[-t, ]
test$drv <- factor(test$drv)  #구동방식 범주형 변환
formula <- hwy ~ disp+cyl+drv
hwy_ctree <- ctree(formula, data=test)
plot(hwy_ctree)

분석 결과 : 엔진 크기가 작으면서 전륜구동(f)이나 후륜(r) 구동 방식인 경우 고속도로 주행거리가 가장 좋고, 
엔진 크기가 크고, 사륜구동 방식이면 실린더 수가 많은 경우 고속도로 주행거리가 적은 것으로 분석된다.
```

![1569205765646](assets/1569205765646.png)

### 2. 연습문제2

```R
install.packages("arules")
library(arules)
data("AdultUCI")
#성인 대상 인구 소득에 관한 설문 조사 데이터
#48,842 관측치와 15개변수
age, workclass(직업 :4개), education(교육수준: 16개), marital-status(결혼상태: 6개), occupation(직업:12개), relationship(관계: 6개), race(인종:아시아계, 백인), sex(성별), capital-gain(자본이득), capital-loss(자본손실), fnlwgt(미지의 변수), hours-per-week(주당 근무시간), native-country(국가), income(소득)

#10,000개 관측치를 샘플링해서
자본이득에 영향을 미치는 변수를 분석하기 위해 
capital-gain, hours-per-week, education-num, race, age, income 변수로만 구성된 데이터프레임을 생성한후 분류모델 생성하고 예측하시오

분석결과 : 
```

